import { JobServiceClient } from "@google-cloud/aiplatform";
import { Storage } from "@google-cloud/storage";
import dotenv from "dotenv";

dotenv.config();

const { GCS_BUCKET_NAME, VERTEX_AI_PROJECT_ID, VERTEX_AI_LOCATION } =
  process.env;

if (!GCS_BUCKET_NAME || !VERTEX_AI_PROJECT_ID || !VERTEX_AI_LOCATION) {
  throw new Error(
    "GCS_BUCKET_NAME, VERTEX_AI_PROJECT_ID, and VERTEX_AI_LOCATION must be set in the environment variables"
  );
}

const storage = new Storage();
const jobServiceClient = new JobServiceClient({
  apiEndpoint: `${VERTEX_AI_LOCATION}-aiplatform.googleapis.com`,
});

interface PromptWithId {
  id: string;
  text: string;
}

/**
 * Gemini 2.0 Flash ëª¨ë¸ì— ë°°ì¹˜ ì˜ˆì¸¡ ìš”ì²­ì„ ë³´ëƒ…ë‹ˆë‹¤.
 * @param prompts - ìš”ì²­í•  í”„ë¡¬í”„íŠ¸ ë°°ì—´ (IDì™€ í…ìŠ¤íŠ¸ ê°ì²´)
 * @returns ìƒì„±ëœ ë°°ì¹˜ ì˜ˆì¸¡ ì‘ì—…ì˜ ID
 */
export async function requestBatchPrediction(
  prompts: PromptWithId[]
): Promise<string> {
  const bucket = storage.bucket(GCS_BUCKET_NAME!);
  const { v4: uuidv4 } = require("uuid");
  const jobUuid = uuidv4();
  const inputFileName = `batch-prompts-${jobUuid}.jsonl`;
  const outputPrefix = `batch-results-${jobUuid}/`;
  const timestamp = new Date().toISOString();

  // 1. í”„ë¡¬í”„íŠ¸ë¥¼ JSONL í˜•ì‹ìœ¼ë¡œ ë³€í™˜
  const jsonlData = prompts
    .map((prompt) => {
      const request = {
        contents: [
          {
            role: "user",
            parts: [{ text: prompt.text }],
          },
        ],
      };
      return JSON.stringify({ request, custom_id: prompt.id });
    })
    .join("\n");

  // 2. ë©”íƒ€ë°ì´í„° íŒŒì¼ ì €ì¥
  const metadata = {
    fileUuid: jobUuid,
    prompts: prompts,
    timestamp: timestamp,
    promptCount: prompts.length,
  };

  const metadataFile = bucket.file(`batch-metadata-${jobUuid}.json`);
  await metadataFile.save(JSON.stringify(metadata, null, 2), {
    contentType: "application/json",
  });

  // 3. JSONL íŒŒì¼ì„ GCSì— ì—…ë¡œë“œ
  const inputFile = bucket.file(inputFileName);
  await inputFile.save(jsonlData, {
    contentType: "application/jsonl",
  });

  console.log(
    `ğŸ“¤ ì…ë ¥ íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ: gs://${GCS_BUCKET_NAME}/${inputFileName}`
  );

  // 4. ë°°ì¹˜ ì˜ˆì¸¡ ì‘ì—… ìƒì„±
  const model = `projects/${VERTEX_AI_PROJECT_ID}/locations/${VERTEX_AI_LOCATION}/publishers/google/models/gemini-2.0-flash-001`;

  const batchPredictionJob = {
    displayName: `gemini-batch-predict-${jobUuid}`,
    model: model,
    inputConfig: {
      instancesFormat: "jsonl",
      gcsSource: {
        uris: [`gs://${GCS_BUCKET_NAME}/${inputFileName}`],
      },
    },
    outputConfig: {
      predictionsFormat: "jsonl",
      gcsDestination: {
        outputUriPrefix: `gs://${GCS_BUCKET_NAME}/${outputPrefix}`,
      },
    },
  };

  const [response] = await jobServiceClient.createBatchPredictionJob({
    parent: `projects/${VERTEX_AI_PROJECT_ID}/locations/${VERTEX_AI_LOCATION}`,
    batchPredictionJob: batchPredictionJob,
  });

  if (!response.name) {
    throw new Error("ë°°ì¹˜ ì˜ˆì¸¡ ì‘ì—… ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.");
  }

  console.log(`âœ… ë°°ì¹˜ ì˜ˆì¸¡ ì‘ì—… ìƒì„± ì™„ë£Œ: ${response.name}`);

  const jobId = response.name.split("/").pop();
  if (!jobId) {
    throw new Error("ì‘ì—… IDë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.");
  }

  return jobId;
}
